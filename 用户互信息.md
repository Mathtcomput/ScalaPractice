# spark统计用户互信息 #
```scala

val inputs=sc.textFile("*****")

val temp=inputs.map(x=>x.split("\u0001")).filter(x=>x.length==16).map(x=>(x(4),x(7),x(9),x(6),x(0).toLong))

val temp2=temp.filter(x=>x._2.split("-").length==4 && x._3.equals("1") && x._4.equals("SEARCH")).map(x=>(x._1,x._2.split("-")(2),x._5)).map(x => (x._1, (x._2, x._3)))

val temp3 = temp2.groupByKey().map(x => (x._1, x._2.toSeq.sortBy(y => y._2).map(y => y._1))).map{x =>
	val origion_list = x._2
	val changed_list = x._2.drop(1)
	val pairs_list = origion_list.zip(changed_list)
	(x._1, pairs_list.filter(y => !y._1.equals(y._2)))}

val temp4 = temp3.flatMap(x => x._2.map(y => (y, 1))).reduceByKey(_ + _)

val temp5 = temp4.map(x=>(Array(x._1._1,x._1._2).sorted,x._2)).map(y => ((y._1(0),y._1(1)),y._2)).reduceByKey(_ + _)

val rdd1 = temp5.map(x => (x._1._1,x._2)).reduceByKey(_ + _)

val rdd2 = temp5.map(x => (x._1._2,x._2)).reduceByKey(_ + _)

val rdd3 = temp5.map(x => (x._1._1,(x._1._2,x._2)))

val rddResult = rdd3.join(rdd1).map(y => (y._2._1._1,(y._1,y._1._2,y._2._2))).join(rdd2).map(z => (z._2._1._1,(z._1,z._2._1._2,z._2._1._3,z._2._2)))

val Result = rddResult.map(x => (x._1,(x._2._1,(x._2._2/727820.toFloat)*log(x._2._2/(x._2._3*x._2._4).toFloat*727820)))).groupByKey().map(x => (x._1,x._2.toSeq.sortWith(_._2>_._2).take(5)))

val outputResult = Result.sample(false,0.001,9).map(x=>s"${x._1}\t${x._2(0)._1}\t${x._2(0)._2}")

outputResult.repartition(1).saveAsTextFile("******")
```
